{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [íŒŒì´ğŸŒ] ì •ê¸°ì„¸ì…˜ 3ì£¼ì°¨  ê³¼ì œ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ë‹¤ìŒ ë¬¸ì¥ì´ ë§ìœ¼ë©´ T, í‹€ë¦¬ë©´ Fë¡œ í‘œì‹œí•˜ì‹œì˜¤.\n",
    "1. ëŒ€ë¶€ë¶„ì´ Positiveì´ê³  Negativeí•œ ë°ì´í„°ê°€ ì¼ë¶€ì¼ ê²½ìš° ì„±ëŠ¥ ì§€í‘œë¡œ ROC-AUCë³´ë‹¤ëŠ” ì •í™•ë„ê°€ ë” ì í•©í•˜ë‹¤. (ã€€)\n",
    "2. ID, ì£¼ë¯¼ë²ˆí˜¸, ì´ë¦„ê³¼ ê°™ì´ ë‹¨ìˆœ ì‹ë³„ì í”¼ì²˜ëŠ” ë°ì´í„° ë¶„ì„ì— ë¶ˆí•„ìš”í•˜ë¯€ë¡œ ë“œë¡­í•˜ëŠ” ê²Œ ì¢‹ë‹¤. (ã€€)\n",
    "3. LightGBMì´ XGBoostë³´ë‹¤ í•™ìŠµì„ í•˜ëŠ”ë° ì‹œê°„ì´ ëœ ê±¸ë¦°ë‹¤. (ã€€)\n",
    "4. ì˜¤ë²„ìƒ˜í”Œë§ ì‹œ ì œëŒ€ë¡œëœ í•™ìŠµì„ ìœ„í•´ ë°ì´í„° ì¦ì‹ì‹œ ë™ì¼í•œ ë°ì´í„°ë¥¼ ë³µì œí•˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì¦ì‹ì‹œí‚¨ë‹¤. (ã€€)\n",
    "5. ìŠ¤íƒœí‚¹ ê¸°ë²•ìœ¼ë¡œ ì˜ˆì¸¡ì„ í• ì‹œ í•­ìƒ ê°œë³„ ëª¨ë¸ë³´ë‹¤ ì„±ëŠ¥ì´ ì¢‹ì•„ì§„ë‹¤. (ã€€)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. ë‹¤ìŒ ë¹ˆì¹¸ì„ ì±„ìš°ì‹œì˜¤.\n",
    "1. ì§€ë„ í•™ìŠµì—ì„œ ê·¹ë‹¨ì ìœ¼ë¡œ ë¶ˆê· í˜„í•­ ë ˆì´ë¸” ê°’ ë¶„í¬ë¥¼ ê°–ê³  ìˆì„ ë•Œ ì ì€ ë°ì´í„° ì„¸íŠ¸ë¥¼ ì¦ì‹í•˜ëŠ” (ã€€ã€€ã€€ã€€) ë°©ë²•ì„ ì‚¬ìš©í•˜ë©´ ì„±ëŠ¥ì„ ë†’ì¼ ìˆ˜ ìˆë‹¤.\n",
    "2. (ã€€ã€€ã€€ã€€)ëŠ” ì ì€ ë°ì´í„° ì„¸íŠ¸ì— ìˆëŠ” ê°œë³„ ë°ì´í„°ë“¤ì˜ (ã€€ã€€ã€€ã€€)ë¥¼ ì°¾ì•„ì„œ Kê°œì˜ ì´ì›ƒë“¤ì˜ ì°¨ì´ë¥¼ ì¼ì • ê°’ìœ¼ë¡œ ë§Œë“¤ì–´ì„œ ê¸°ì¡´ ë°ì´í„°ì™€ ì•½ê°„ ì°¨ì´ê°€ ë‚˜ëŠ” ìƒˆë¡œìš´ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë°©ì‹ì´ë‹¤.\n",
    "3. (ã€€ã€€ã€€ã€€)ì€ ê°œë³„ ì•Œê³ ë¦¬ì¦˜ì˜ ì˜ˆì¸¡ ê²°ê³¼ ë°ì´í„° ì„¸íŠ¸ë¥¼ ìµœì¢…ì ì¸ ë©”íƒ€ ë°ì´í„° ì„¸íŠ¸ë¡œ ë§Œë“¤ì–´ ë³„ë„ì˜ MLì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì¹˜ì¢… í•™ìŠµì„ ìˆ˜í–‰í•˜ê³  í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ë‹¤ì‹œ ìµœì¢… ì˜ˆì¸¡ì„ í•˜ëŠ” ë°©ì‹ì´ë‹¤.\n",
    "4. ê¸°ë³¸ ìŠ¤íƒœí‚¹ ëª¨ë¸ì˜ ê²½ìš° í…ŒìŠ¤íŠ¸ìš© ë ˆì´ë¸” ë°ì´í„° ì„¸íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµì„ í•˜ê¸°ì— (ã€€ã€€ã€€ã€€) ë¬¸ì œë¥¼ ë°œìƒì‹œí‚¬ ìˆ˜ ìˆìœ¼ë©° ì´ë¥¼ í•´ê²° í•˜ê¸° ìœ„í•´ (ã€€ã€€ã€€ã€€)ìœ¼ë¡œ í•™ìŠµìš© ìŠ¤íƒœí‚¹ ë°ì´í„°ì™€ í…ŒìŠ¤íŠ¸ìš© ìŠ¤íƒœí‚¹ ë°ì´í„°ë¡œ ë§Œë“  ë’¤ ì´ê²ƒìœ¼ë¡œ í•™ìŠµê³¼ ì˜ˆì¸¡ì„ í•˜ë©´ ëœë‹¤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. XGBoostì™€ LightGBM ì¤‘ ì–´ëŠ ê²ƒì´ ë¹ ë¥¸ì§€ ë¹„êµí•´ë³´ì."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import time\n",
    "\n",
    "cust_df = pd.read_csv(\"./train_santander.csv\",encoding='latin-1')\n",
    "print('dataset shape:', cust_df.shape)\n",
    "cust_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cust_df['var3'].replace(-999999,2, inplace=True)\n",
    "cust_df.drop('ID',axis=1 , inplace=True)\n",
    "\n",
    "X_features = cust_df.iloc[:, :-1]\n",
    "y_labels = cust_df.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels,\n",
    "                                                    test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. XGBoostë¡œ í•™ìŠµì„ í•  ê²½ìš° ê±¸ë¦° ì‹œê°„ì„ ì ìœ¼ì‹œì˜¤.\n",
    "XGBoostë¡œ í•™ìŠµí–ˆì„ ë•Œ ê±¸ë¦° ì‹œê°„ : (ã€€ã€€ã€€ã€€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# ì‹œì‘ ì‹œê°„ì„ ì„¤ì •í•˜ëŠ” ì½”ë“œë¥¼ ì ìœ¼ì‹œì˜¤.\n",
    "\n",
    "xgb_clf = XGBClassifier(n_estimators=500, random_state=156)\n",
    "\n",
    "xgb_clf.fit(X_train, y_train, early_stopping_rounds=100,\n",
    "            eval_metric=\"auc\", eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "# ê±¸ë¦° ì‹œê°„ì„ ê³„ì‚°í•˜ëŠ” ì½”ë“œë¥¼ ì ìœ¼ì‹œì˜¤.\n",
    "\n",
    "print('XGBoost ìˆ˜í–‰ ì‹œê°„ : {0:.1f}'.format(xgb_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2. LightGBMìœ¼ë¡œ í•™ìŠµì„ í•  ê²½ìš° ê±¸ë¦° ì‹œê°„ì„ ì ìœ¼ì‹œì˜¤.\n",
    "LightGBMìœ¼ë¡œ í•™ìŠµí–ˆì„ ë•Œ ê±¸ë¦° ì‹œê°„ : (ã€€ã€€ã€€ã€€)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# ì‹œì‘ ì‹œê°„ì„ ì„¤ì •í•˜ëŠ” ì½”ë“œë¥¼ ì ìœ¼ì‹œì˜¤.\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=500)\n",
    "\n",
    "evals = [(X_test, y_test)]\n",
    "lgbm_clf.fit(X_train, y_train, early_stopping_rounds=100, eval_metric=\"auc\", eval_set=evals,\n",
    "                verbose=True)\n",
    "\n",
    "# ê±¸ë¦° ì‹œê°„ì„ ê³„ì‚°í•˜ëŠ” ì½”ë“œë¥¼ ì ìœ¼ì‹œì˜¤.\n",
    "\n",
    "print('XGBoost ìˆ˜í–‰ ì‹œê°„ : {0:.1f}'.format(lgbm_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. XGBoostì™€ LightGBM ì¤‘ ì–´ëŠ ê²ƒì´ ë” ë¹ ë¥¸ì§€ ì“°ì‹œì˜¤.\n",
    "(ã€€ã€€ã€€ã€€ã€€ã€€ã€€ã€€)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. ë‹¤ìŒì€ ì‹ ìš©ì¹´ë“œ ë°ì´í„° ì„¸íŠ¸ì´ë‹¤. ì´ ë°ì´í„° ì„¸íŠ¸ì˜ ì´ìƒì¹˜ë¥¼ ì œê±°í–ˆì„ ë•Œì™€ SMOTEë°©ì‹ì„ ì‚¬ìš©í–ˆì„ ë•Œ ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ ìˆ˜ì¹˜ê°€ ë³€í™”í•˜ì˜€ë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline\n",
    "\n",
    "card_df = pd.read_csv('./creditcard.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    df_copy.drop('Time', axis=1, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_dataset(df=None):\n",
    "    df_copy = get_preprocessed_df(df)\n",
    "    X_features = df_copy.iloc[:, :-1]\n",
    "    y_target = df_copy.iloc[:, -1]\n",
    "    X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X_features, y_target, test_size=0.3, random_state=0, stratify=y_target)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = get_train_test_dataset(card_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def get_clf_eval(y_test, pred):\n",
    "    confusion = confusion_matrix( y_test, pred)\n",
    "    accuracy = accuracy_score(y_test , pred)\n",
    "    precision = precision_score(y_test , pred)\n",
    "    recall = recall_score(y_test , pred)\n",
    "    f1 = f1_score(y_test,pred)\n",
    "    \n",
    "    print('ì˜¤ì°¨ í–‰ë ¬')\n",
    "    print(confusion)\n",
    "    print('ì •í™•ë„: {0:.4f}, ì •ë°€ë„: {1:.4f}, ì¬í˜„ìœ¨: {2:.4f},\\\n",
    "    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_train_eval(model, ftr_train=None, ftr_test=None, tgt_train=None, tgt_test=None):\n",
    "    model.fit(ftr_train, tgt_train)\n",
    "    pred = model.predict(ftr_test)\n",
    "    get_clf_eval(tgt_test, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "print('='*15+' ë°ì´í„° ê°€ê³µ ì „ì˜ LightGBM ì„±ëŠ¥ ì§€í‘œ '+'='*15)\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_outlier(df=None, column=None, weight=1.5):\n",
    "    fraud = df[df['Class']==1][column]\n",
    "    quantile_25 = np.percentile(fraud.values, 25)\n",
    "    quantile_75 = np.percentile(fraud.values, 75)\n",
    "    boundary = quantile_75 - quantile_25\n",
    "    iqr_weight = boundary * weight\n",
    "    lowest_val = quantile_25 - iqr_weight\n",
    "    highest_val = quantile_75 + iqr_weight\n",
    "    \n",
    "    outlier_index = fraud[(fraud < lowest_val) | (fraud > highest_val)].index\n",
    "    return outlier_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_index = get_outlier(df=card_df, column='V14', weight=1.5)\n",
    "print('ì´ìƒì¹˜ ë°ì´í„° ì¸ë±ìŠ¤:', outlier_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preprocessed_df(df=None):\n",
    "    df_copy = df.copy()\n",
    "    amount_n = np.log1p(df_copy['Amount'])\n",
    "    df_copy.insert(0, 'Amount_Scaled', amount_n)\n",
    "    df_copy.drop(['Time','Amount'], axis=1, inplace=True)\n",
    "    outlier_index = get_outlier(df=df_copy, column='V14', weight=1.5)\n",
    "    df_copy.drop(outlier_index, axis=0, inplace=True)\n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('='*15+' V14ì˜ ì´ìƒì¹˜ ì œê±° í›„ LightGBM ì„±ëŠ¥ ì§€í‘œ '+'='*15)\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train, ftr_test=X_test, tgt_train=y_train, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. V14ì˜ ì´ìƒì¹˜ ì œê±°ì— ëŒ€í•˜ì—¬ ë‹¤ìŒì˜ ë¬¼ìŒì— ë‹µí•˜ì‹œì˜¤.\n",
    "1. V14ì˜ ì´ìƒì¹˜ë¥¼ ì œê±°í•œ í›„ ê°€ê³µ ì „ ë°ì´í„°ì™€ ë¹„êµí–ˆì„ ë•Œ ì •ë°€ë„ëŠ” ì–´ë–»ê²Œ ë˜ì—ˆëŠ”ê°€?\n",
    "2. V14ì˜ ì´ìƒì¹˜ë¥¼ ì œê±°í•œ í›„ ê°€ê³µ ì „ ë°ì´í„°ì™€ ë¹„êµí–ˆì„ ë•Œ ì¬í˜„ìœ¨ëŠ” ì–´ë–»ê²Œ ë˜ì—ˆëŠ”ê°€?\n",
    "3. ì™œ ê·¸ë ‡ê²Œ ë³€í–ˆëŠ”ì§€ ì„œìˆ í•˜ì‹œì˜¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE(random_state=0)\n",
    "X_train_over, y_train_over = smote.fit_sample(X_train, y_train)\n",
    "\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1000, num_leaves=64, n_jobs=-1, boost_from_average=False)\n",
    "print('='*15+' SMOTEë°©ë²• ì‚¬ìš© í›„ ì„±ëŠ¥ ì§€í‘œ '+'='*15)\n",
    "get_model_train_eval(lgbm_clf, ftr_train=X_train_over, ftr_test=X_test,\n",
    "                  tgt_train=y_train_over, tgt_test=y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. SMOTE ë°©ì‹ì„ ì‚¬ìš©í•œ í›„ì— ëŒ€í•˜ì—¬ ë‹¤ìŒì˜ ë¬¼ìŒì— ë‹µí•˜ì‹œì˜¤.\n",
    "1. SMOTE ë°©ì‹ì„ ì‚¬ìš©í•œ í›„ ê°€ê³µ ì „ ë°ì´í„°ì™€ ë¹„êµí–ˆì„ ë•Œ ì •ë°€ë„ëŠ” ì–´ë–»ê²Œ ë˜ì—ˆëŠ”ê°€?\n",
    "2. SMOTE ë°©ì‹ì„ ì‚¬ìš©í•œ í›„ ê°€ê³µ ì „ ë°ì´í„°ì™€ ë¹„êµí–ˆì„ ë•Œ ì¬í˜„ìœ¨ëŠ” ì–´ë–»ê²Œ ë˜ì—ˆëŠ”ê°€?\n",
    "3. ì™œ ê·¸ë ‡ê²Œ ë³€í–ˆëŠ”ì§€ ì„œìˆ í•˜ì‹œì˜¤."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
